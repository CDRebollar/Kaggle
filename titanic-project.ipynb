{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7304665,"sourceType":"datasetVersion","datasetId":4238134}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/carlosdrebollar/titanic-project?scriptVersionId=157182475\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # data visualization\nfrom sklearn.model_selection import train_test_split # train/validation splits\nfrom sklearn.impute import KNNImputer, SimpleImputer # imputers \n\n# Load data\ntrain_df = pd.read_csv(\"/kaggle/input/titanic-machine-learning-from-disaster/train.csv\")\n\n# View head of training data\ntrain_df.head()\n\n# Analyze missing values\ntrain_df.isna().sum(axis = 0) # Age: 177, Cabin: 687, Embarked: 2\n\n# Removed 2 obs w/ missing \"Embarked\"\ntrain_df = train_df.dropna(subset = 'Embarked') \ntrain_df.shape # 889 observations, 12 variables\n\n# Add \"Cabin\" and \"Age\" missing indicator\ntrain_df[\"Cabin_NA\"] = train_df[\"Cabin\"].isna()\ntrain_df[\"Age_NA\"] = train_df[\"Age\"].isna()\n\n# Add \"Alone\" indicator\ntrain_df[\"Alone\"] = train_df[[\"SibSp\", \"Parch\"]].sum(axis = 1) == 0\n\n# Add \"Adult_male\" indicator\ntrain_df[\"Adult_male\"] = (train_df[\"Age\"] >= 18) & (train_df[\"Sex\"] == \"male\")\n\n# Define a regular expression to extract the prefix and number\nregex_pattern = r'(?P<ticket_prefix>.*?)(?P<ticket_number>\\d+)$'\n\n# Use str.extract to create new columns\ntrain_df = pd.merge(train_df, train_df['Ticket'].str.extract(regex_pattern), left_index = True, right_index = True)\ntrain_df['ticket_prefix'] = train_df['ticket_prefix'].replace(\"\", 'NA').str.replace(\".\", \"\")\ntrain_df['ticket_prefix'] = train_df['ticket_prefix'].str.replace(\" \", \"\").str[:1]\n\n# Extracting title into a new column\ntrain_df[\"Name_title\"] = train_df[\"Name\"].str.split(',').str[1].str.strip().str.split(' ').str[0]\n\n# List of valid titles\nvalid_titles = ['Mrs.', 'Mr.', 'Master.', 'Miss.']\n\n# Replace titles not in the valid list with 'Other'\ntrain_df[\"Name_title\"] = train_df[\"Name_title\"].apply(lambda x: x if x in valid_titles else 'Other')\n\n# Calculate the median age for each title\nmedian_age_by_title = train_df.groupby('Name_title')['Age'].transform('median')\n\n# Fill missing values in 'Age' with the corresponding median for each title\ntrain_df[\"Age_IMP\"] = train_df['Age'].fillna(median_age_by_title)\n\n# Define initial data sets\nX = train_df.drop([\"Survived\", \"PassengerId\", \"Name\", \"Cabin\", \"Ticket\", \"ticket_number\", \"Age\"], axis = 1)\ny = train_df.Survived\n\n# Split into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = .15, random_state = 42)\n\n# Get dummies\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\n\nprint(X_train.columns)\n\n# Initialize KNN imputer\nknn_imputer = KNNImputer()\nknn_imputer.fit(X_train)\n\n# Initialize simple median imputer\nmedian_imputer = SimpleImputer(strategy = 'median')\nmedian_imputer.fit(X_train)\n\n# Impute Age with median imputer for training and validation\nX_train_imp = pd.DataFrame(median_imputer.transform(X_train), columns = X_train.columns.tolist())\nX_valid_imp = pd.DataFrame(median_imputer.transform(X_valid), columns = X_valid.columns.tolist())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-31T06:50:51.622162Z","iopub.execute_input":"2023-12-31T06:50:51.622601Z","iopub.status.idle":"2023-12-31T06:50:52.251093Z","shell.execute_reply.started":"2023-12-31T06:50:51.622568Z","shell.execute_reply":"2023-12-31T06:50:52.249616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression\n## Validation Accuracy = 85.82%\n### Data preparation","metadata":{}},{"cell_type":"code","source":"def LogPrep(input_dat):\n    \n    dat = input_dat.copy()\n\n    dat[\"Pclass\"] = dat[\"Pclass\"].astype(\"category\")\n\n    # Only 15 with \"Parch\" >= 3. Change to categorical\n    dat[\"Parch\"] = dat[\"Parch\"].apply(lambda x: 3 if x >= 3 else x).astype(str)\n    dat[\"Parch\"] = dat[\"Parch\"].replace(\"3\", \"3 or greater\").astype(\"category\")\n\n    # Only 12 with \"SibSp\" >= 5. Change to categorical\n    dat[\"SibSp\"] = dat[\"SibSp\"].apply(lambda x: 5 if x >= 5 else x).astype(str)\n    dat[\"SibSp\"] = dat[\"SibSp\"].replace(\"5\", \"5 or greater\").astype(\"category\")\n    \n    dat = dat.drop([\"Sex_female\", \"Embarked_C\"], axis = 1)\n    dat = pd.get_dummies(dat, drop_first = True)\n    \n    return(dat)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T06:51:01.016272Z","iopub.execute_input":"2023-12-31T06:51:01.016926Z","iopub.status.idle":"2023-12-31T06:51:01.02529Z","shell.execute_reply.started":"2023-12-31T06:51:01.016891Z","shell.execute_reply":"2023-12-31T06:51:01.024052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Fitting","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\n# Prepare training and validation data\nX_train_imp_lr = LogPrep(X_train_imp)\nX_valid_imp_lr = LogPrep(X_valid_imp)\n\n# Create a Logistic Regression model\nlogreg_model = LogisticRegression(max_iter = 2000, random_state = 42)\n\n# Define the hyperparameter grid to search\nparam_grid = {\n    'penalty': ['l1', 'l2'],  # Regularization type\n    'solver': ['lbfgs', 'liblinear'],\n    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # Inverse of regularization strength\n}\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(logreg_model, param_grid, cv = 5, scoring = 'roc_auc')\n\n# Fit the model to the training data\ngrid_search.fit(X_train_imp_lr, y_train)\n\n# Print the best hyperparameters found\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\nprint(\"Best Average AUC Score:\", round(100 * grid_search.best_score_, 2))\n\n# Store the best model\nbest_logreg_model = grid_search.best_estimator_\n\n# Make predictions on the training set to get predicted probabilities\ny_pred_proba = best_logreg_model.predict_proba(X_train_imp_lr)[:, 1]\n\n# Calculate AUC on training: 86.57%\nauc_score = roc_auc_score(y_train, y_pred_proba)\nprint(\"Training AUC:\", round(100 * auc_score, 2))\n\n# Make predictions on the validation set to get predicted probabilities\ny_pred_proba_valid = best_logreg_model.predict_proba(X_valid_imp_lr)[:, 1]\n\n# Calculate AUC on validation: 88.26%\nauc_score_valid = roc_auc_score(y_valid, y_pred_proba_valid)\nprint(\"Validation AUC:\", round(100 * auc_score_valid, 2))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:03:55.04135Z","iopub.execute_input":"2023-12-31T07:03:55.041769Z","iopub.status.idle":"2023-12-31T07:04:05.932381Z","shell.execute_reply.started":"2023-12-31T07:03:55.041737Z","shell.execute_reply":"2023-12-31T07:04:05.930588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Determine the cutoff that maximized the accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nlr_results = pd.DataFrame()\nlr_results['p_hat'] = best_logreg_model.predict_proba(X_train_imp_lr)[:, 1]\n\naccuracy = np.array([])\n\nfor cutoff in range(100):\n  lr_results[\"pred\"] = lr_results['p_hat'].map(lambda x: 1 if x > (cutoff / 100) else 0)\n  value_a = accuracy_score(y_train, lr_results[\"pred\"])\n  accuracy = np.append(accuracy, value_a)\n\ndata = {'Accuracy': accuracy, 'Cut-off': range(100)}\nacc_s = pd.DataFrame(data)\n\n# Retrieve cutoff that maximizes accuracy\noptimal_cutoff = acc_s.sort_values(by = ['Accuracy'], ascending = False).iloc[0][1]/100\nprint(optimal_cutoff)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:04:11.967075Z","iopub.execute_input":"2023-12-31T07:04:11.967509Z","iopub.status.idle":"2023-12-31T07:04:12.23301Z","shell.execute_reply.started":"2023-12-31T07:04:11.967476Z","shell.execute_reply":"2023-12-31T07:04:12.231561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Determine accuracy on validation set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Make predictions on the validation set to get predicted probabilities\ny_prob_valid_lr = best_logreg_model.predict_proba(X_valid_imp_lr)[:,1]\ny_pred_valid_lr = y_prob_valid_lr >= optimal_cutoff\n\naccuracy_lr = accuracy_score(y_valid, y_pred_valid_lr)\nprint(\"Accuracy:\", round(100 * accuracy_lr, 2)) # Accuracy of 85.82% on validation set.","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:04:22.346855Z","iopub.execute_input":"2023-12-31T07:04:22.347239Z","iopub.status.idle":"2023-12-31T07:04:22.360613Z","shell.execute_reply.started":"2023-12-31T07:04:22.347209Z","shell.execute_reply":"2023-12-31T07:04:22.35901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tree-Based Methods\n## Decision Tree \n### Cross-validated accuracy on training = 82.65%\n### Validation Accuracy = 81.34%","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Create a DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(random_state = 42)\n\n# Define the hyperparameter grid to search\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [None, 3, 4, 5],\n    'min_samples_split': [2, 3, 4],\n    'min_samples_leaf': [5, 6, 7, 8, 9, 10, 11, 12]\n}\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(dt_classifier, param_grid, cv = 5, scoring = 'accuracy')\n\n# Fit the model to the training data\ngrid_search.fit(X_train_imp, y_train)\n\n# Print the best hyperparameters found\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Print the best accuracy score found (on training)\nprint(\"Best cross-validated training accuracy:\", round(100 * grid_search.best_score_, 2))\n\n# Store best model\nbest_dt_model = grid_search.best_estimator_\n\n# Store validation predictions\ny_pred_valid_dt = best_dt_model.predict(X_valid_imp)\n\n# Evaluate the model's performance\naccuracy_dt = best_dt_model.score(X_valid_imp, y_valid) * 100\nprint(\"Accuracy:\", round(accuracy_dt, 2)) # Accuracy of 81.46% on validation set.","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:04:35.821035Z","iopub.execute_input":"2023-12-31T07:04:35.821444Z","iopub.status.idle":"2023-12-31T07:04:42.665181Z","shell.execute_reply.started":"2023-12-31T07:04:35.821412Z","shell.execute_reply":"2023-12-31T07:04:42.663171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest\n### Mean training accuracy in cross-validation = 83.18%\n### Validation accuracy = 84.33%","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Create a RandomForestClassifier\nrf_classifier = RandomForestClassifier(random_state = 42)\n\n# Define the hyperparameter grid to search\nparam_grid = {\n    'n_estimators': [100],  # Number of trees in the forest\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [3, 9, 10, 11],\n    'min_samples_split': [9, 10, 11],\n    'min_samples_leaf': [2, 3, 4]\n}\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(rf_classifier, param_grid, cv = 5, scoring = 'accuracy')\n\n# Fit the model to the training data\ngrid_search.fit(X_train_imp, y_train)\n\n# Print the best hyperparameters found\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Print the best accuracy score found (on training)\nprint(\"Best cross-validated training accuracy:\", round(100 * grid_search.best_score_, 2))\n\n# Store the best model\nbest_rf_model = grid_search.best_estimator_\n\n# Store validation predictions\ny_pred_valid_rf = best_rf_model.predict(X_valid_imp)\ny_prob_valid_rf = best_rf_model.predict_proba(X_valid_imp)[:,1]\n\n\n# Evaluate the model's performance\naccuracy_rf = best_rf_model.score(X_valid_imp, y_valid) * 100\nprint(\"Accuracy:\", round(accuracy_rf, 2)) # Display accuracy on the validation set","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:04:59.711542Z","iopub.execute_input":"2023-12-31T07:04:59.712899Z","iopub.status.idle":"2023-12-31T07:06:33.31169Z","shell.execute_reply.started":"2023-12-31T07:04:59.712857Z","shell.execute_reply":"2023-12-31T07:06:33.310202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Classifier \n### Mean training accuracy in cross-validation = 83.71%\n### Validation accuracy = 79.85%","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n# Create an XGBClassifier\nxgb_classifier = XGBClassifier(random_state = 42)\n\n# Define the hyperparameter grid to search\nparam_grid = {\n    'n_estimators': [13, 14, 15],  # Number of boosting rounds\n    'learning_rate': [.1, .2, .3],\n    'max_depth': [9, 10, 11],\n    'subsample': [.5, .6, .7],\n    'colsample_bytree': [0.8, 1.0],\n    'gamma': [0, 1, 5],\n    'min_child_weight': [1, 3]\n}\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(xgb_classifier, param_grid, cv = 5, scoring = 'accuracy')\n\n# Fit the model to the training data\ngrid_search.fit(X_train_imp, y_train)\n\n# Print the best hyperparameters found\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Print the best accuracy score found (on training)\nprint(\"Best cross-validated training accuracy:\", round(100 * grid_search.best_score_, 2))\n\n# Store the best model\nbest_xgb_model = grid_search.best_estimator_\n\n# Store validation predictions\ny_pred_valid_xgb = best_xgb_model.predict(X_valid_imp)\ny_prob_valid_xgb = best_xgb_model.predict_proba(X_valid_imp)[:,1]\n\n# Evaluate the model's performance\naccuracy_xgb = best_xgb_model.score(X_valid_imp, y_valid) * 100\nprint(\"Accuracy:\", round(accuracy_xgb, 2))  # Display accuracy on the validation set","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:14:15.001957Z","iopub.execute_input":"2023-12-31T07:14:15.002407Z","iopub.status.idle":"2023-12-31T07:17:43.662383Z","shell.execute_reply.started":"2023-12-31T07:14:15.002371Z","shell.execute_reply":"2023-12-31T07:17:43.661113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes\n### Mean training accuracy in cross-validation = 80.26%\n### Validation accuracy = 81.34%","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngnb_classifier = GaussianNB()\n\n# Define the hyperparameter grid\nparam_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(estimator = gnb_classifier, param_grid = param_grid, cv = 5, scoring = 'accuracy')\ngrid_search.fit(X_train_imp, y_train)\n\n# Print the best parameters and corresponding accuracy\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best Accuracy: \", grid_search.best_score_)\n\n# Store the best model\nbest_gnb_model = grid_search.best_estimator_\n\n# Store validation predictions\ny_pred_valid_gnb = best_gnb_model.predict(X_valid_imp)\ny_prob_valid_gnb = best_gnb_model.predict_proba(X_valid_imp)[:,1]\n\naccuracy_nb = accuracy_score(y_valid, y_pred_valid_gnb)\nprint(\"Validation accuracy:\", round(100 * accuracy_nb, 2))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:53:08.626876Z","iopub.execute_input":"2023-12-31T07:53:08.627432Z","iopub.status.idle":"2023-12-31T07:53:08.846744Z","shell.execute_reply.started":"2023-12-31T07:53:08.627388Z","shell.execute_reply":"2023-12-31T07:53:08.845936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN Classifier\n### Mean training accuracy in cross-validation = 82.38%\n### Validation accuracy = 82.84%","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scale the features using StandardScaler\nscaler_cont = StandardScaler().fit(X_train_imp)\nX_train_knn = scaler_cont.transform(X_train_imp)\nX_valid_knn = scaler_cont.transform(X_valid_imp)\n\n# Cross validation to get best value of K\nk_values = [i for i in range (1,31)]\nscores = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    score = cross_val_score(knn, X_train_knn, y_train, cv = 6)\n    scores.append(np.mean(score))\n    \nknn_dat = pd.DataFrame({\"k_values\": k_values, \"scores\": scores}) \n\n# Plot results\nsns.lineplot(x = k_values, y = knn_dat[\"scores\"], marker = 'o')\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy Score\")\n\n# Extract best k\nprint(knn_dat.sort_values(\"scores\", ascending = False).head(1))\nbest_k = knn_dat.sort_values(\"scores\", ascending = False).iloc[0]['k_values']\n\n# Instantiate and fit the model\nknn = KNeighborsClassifier(n_neighbors = int(best_k))\nknn.fit(X_train_knn, y_train)\n\n# Store validation predictions\ny_pred_valid_knn = knn.predict(X_valid_knn)\ny_prob_valid_knn = knn.predict_proba(X_valid_knn)[:,1]\n\n# Accuracy\naccuracy_knn = accuracy_score(y_valid, y_pred_valid_knn)\nprint(\"Validation accuracy:\", round(100 * accuracy_knn , 2))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:07:29.311318Z","iopub.execute_input":"2023-12-31T07:07:29.311796Z","iopub.status.idle":"2023-12-31T07:07:32.838411Z","shell.execute_reply.started":"2023-12-31T07:07:29.311761Z","shell.execute_reply":"2023-12-31T07:07:32.83699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine\n### Mean training accuracy in cross-validation = 82.65%\n### Validation accuracy = 84.33%","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Scale the features using StandardScaler\nscaler_cat = StandardScaler().fit(X_train_imp_lr)\nX_train_svm = scaler_cat.transform(X_train_imp_lr)\nX_valid_svm = scaler_cat.transform(X_valid_imp_lr)\n\n# Initialize SVC\nsvm_classifier = SVC(probability = True, random_state = 42)\n\n# Define the hyperparameter grid\nparam_grid = {'C': [.5, 1, 10, 100],\n             'gamma': ['scale', 1, .1, .01, .001, .0001],\n             'kernel': ['rbf']}\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(svm_classifier, param_grid = param_grid, cv = 5, scoring = 'accuracy')\ngrid_search.fit(X_train_svm, y_train)\n\n# Print the best parameters and corresponding accuracy\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best Accuracy: \", grid_search.best_score_)\n\n# Store the best model\nbest_svm_model = grid_search.best_estimator_\n\n# Store validation predictions\ny_pred_valid_svm = best_svm_model.predict(X_valid_svm)\ny_prob_valid_svm = best_svm_model.predict_proba(X_valid_svm)[:,1]\n\naccuracy_svm = accuracy_score(y_valid, y_pred_valid_svm)\nprint(\"Validation accuracy:\", round(100 * accuracy_svm, 2))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:25:12.176448Z","iopub.execute_input":"2023-12-31T07:25:12.176901Z","iopub.status.idle":"2023-12-31T07:25:26.210983Z","shell.execute_reply.started":"2023-12-31T07:25:12.176868Z","shell.execute_reply":"2023-12-31T07:25:26.209527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Nets\n## Sequential Model (Validation accuracy = 84.33%)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Build the neural network model\nmodel = Sequential()\nmodel.add(Dense(units = 64, activation = 'relu', input_dim = X_train_nn.shape[1]))\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Train the model\nmodel.fit(X_train_svm, y_train, epochs = 15, batch_size = 32, validation_data = (X_valid_svm, y_valid))\n\n# Store validation predictions\ny_prob_valid_seq = model.predict(X_valid_svm)\ny_pred_valid_seq = y_prob_valid_seq > .5\n\n# Evaluate the model on the test set\nloss, accuracy = model.evaluate(X_valid_svm, y_valid)\nprint(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:37:00.161364Z","iopub.execute_input":"2023-12-31T07:37:00.161834Z","iopub.status.idle":"2023-12-31T07:37:02.825218Z","shell.execute_reply.started":"2023-12-31T07:37:00.161799Z","shell.execute_reply":"2023-12-31T07:37:02.823985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MLP Classifier\n### Mean training accuracy in cross-validation = 82.25%\n### Validation accuracy = 82.84%","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameter grid to search\nparam_grid = {\n    'hidden_layer_sizes': [2, 3, 4, 5, 6],\n    'alpha': [0.00005, 0.0005, .005],\n    'solver': ['lbfgs']\n}\n\nmlp_model = MLPClassifier(max_iter = 5000, random_state = 42)\n\n# Train the model\ngrid_search = GridSearchCV(mlp_model, param_grid = param_grid, cv = 5)\ngrid_search.fit(X_train_svm, y_train)\n\n# Print the best parameters and corresponding accuracy\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best Accuracy: \", grid_search.best_score_)\n\n# Store the best model\nbest_mlp_model = grid_search.best_estimator_\n\n# Store validation predictions\ny_pred_valid_mlp = best_mlp_model.predict(X_valid_svm)\ny_prob_valid_mlp = best_mlp_model.predict_proba(X_valid_svm)[:,1]\n\n# Accuracy\naccuracy_mlp = accuracy_score(y_valid, y_pred_valid_mlp)\nprint(\"Validation accuracy:\", round(100 * accuracy_mlp , 2))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:52:10.192344Z","iopub.execute_input":"2023-12-31T07:52:10.192829Z","iopub.status.idle":"2023-12-31T07:52:53.642034Z","shell.execute_reply.started":"2023-12-31T07:52:10.192792Z","shell.execute_reply":"2023-12-31T07:52:53.641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Predictions (Validation Accuracy = 84.33%)","metadata":{}},{"cell_type":"code","source":"ensemble = y_valid.to_frame().reset_index()\nensemble['logistic_reg'] = y_prob_valid_lr\nensemble['decision_tree'] = y_pred_valid_dt\nensemble['random_forest'] = y_prob_valid_rf\nensemble['xgboost'] = y_prob_valid_xgb\nensemble['naive_bayes'] = y_pred_valid_gnb\nensemble['knn'] = y_prob_valid_knn\nensemble['svm'] = y_prob_valid_svm\nensemble['sequential_nn'] = y_prob_valid_seq\nensemble['mlp_classifier'] = y_prob_valid_mlp\n\n# Obtain ensemble predictions for validation set\nensemble[\"ens_probs\"] = ensemble.drop([\"index\", \"Survived\"], axis = 1).mean(axis = 1)\nensemble[\"preds\"] = ensemble[\"ens_probs\"] >= .5\n\n# Accuracy\naccuracy_ensemble = accuracy_score(y_valid, ensemble[\"preds\"])\nprint(\"Accuracy:\", round(100 * accuracy_ensemble, 2)) # Accuracy of 84.33% on validation set.","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:53:23.047315Z","iopub.execute_input":"2023-12-31T07:53:23.047766Z","iopub.status.idle":"2023-12-31T07:53:23.06968Z","shell.execute_reply.started":"2023-12-31T07:53:23.047731Z","shell.execute_reply":"2023-12-31T07:53:23.068011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create final predictions on test data\n## Prepare test data","metadata":{}},{"cell_type":"code","source":"# Load data\ntest_df = pd.read_csv(\"/kaggle/input/titanic-machine-learning-from-disaster/test.csv\")\n\n# Add \"Cabin\" and \"Age\" missing indicator\ntest_df[\"Cabin_NA\"] = test_df[\"Cabin\"].isna()\ntest_df[\"Age_NA\"] = test_df[\"Age\"].isna()\n\n# Add \"Alone\" indicator\ntest_df[\"Alone\"] = test_df[[\"SibSp\", \"Parch\"]].sum(axis = 1) == 0\n\n# Add \"Adult_male\" indicator\ntest_df[\"Adult_male\"] = (test_df[\"Age\"] >= 18) & (test_df[\"Sex\"] == \"male\")\n\n# Use str.extract to create new columns\ntest_df = pd.merge(test_df, test_df['Ticket'].str.extract(regex_pattern), left_index = True, right_index = True)\ntest_df['ticket_prefix'] = test_df['ticket_prefix'].replace(\"\", 'NA').str.replace(\".\", \"\")\ntest_df['ticket_prefix'] = test_df['ticket_prefix'].str.replace(\" \", \"\").str[:1]\n\n# Extracting title into a new column\ntest_df[\"Name_title\"] = test_df[\"Name\"].str.split(',').str[1].str.strip().str.split(' ').str[0]\n\n# Replace titles not in the valid list with 'Other'\ntest_df[\"Name_title\"] = test_df[\"Name_title\"].apply(lambda x: x if x in valid_titles else 'Other')\n\n# Calculate the median age for each title\nmedian_age_by_title = test_df.groupby('Name_title')['Age'].transform('median')\n\n# Fill missing values in 'Age' with the corresponding median for each title\ntest_df[\"Age_IMP\"] = test_df['Age'].fillna(median_age_by_title)\n\n# Define initial data sets\nX_test = test_df.drop([\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\", \"ticket_number\", \"Age\"], axis = 1)\n\n# Get dummies\nX_test = pd.get_dummies(X_test).drop([\"ticket_prefix_L\"], axis = 1)\n\n# Impute Age with median imputer for test\nX_test_imp = pd.DataFrame(median_imputer.transform(X_test), columns = X_test.columns.tolist())\n\n# Prepare test for logistic\nX_test_imp_lr = LogPrep(X_test_imp)\n\n# Prepare test for KNN classifier\nX_test_knn = scaler_cont.transform(X_test_imp)\n\n# Prepare test for SVM, Sequential, NN, MLP classifier\nX_test_svm = scaler_cat.transform(X_test_imp_lr)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:53:28.176546Z","iopub.execute_input":"2023-12-31T07:53:28.177881Z","iopub.status.idle":"2023-12-31T07:53:28.260049Z","shell.execute_reply.started":"2023-12-31T07:53:28.177832Z","shell.execute_reply":"2023-12-31T07:53:28.25848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create test set predictions","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\ny_pred_test_lr = best_logreg_model.predict_proba(X_test_imp_lr)[:,1]\n\n# Decision Tree\ny_pred_test_dt = best_dt_model.predict_proba(X_test_imp)[:,1]\n\n# Random Forest\ny_pred_test_rf = best_rf_model.predict_proba(X_test_imp)[:,1]\n\n# XGBoost\ny_pred_test_xgb = best_xgb_model.predict_proba(X_test_imp)[:,1]\n\n# Naive Bayes\ny_pred_test_gnb = best_gnb_model.predict(X_test_imp)\n\n# KNN \ny_pred_test_knn = knn.predict(X_test_knn)\n\n# SVM\ny_pred_test_svm = best_svm_model.predict(X_test_svm)\n\n# Sequential NN\ny_pred_test_seq = model.predict(X_test_svm) > .5\n\n# MLP Classifier\ny_pred_test_mlp = best_mlp_model.predict_proba(X_test_svm)[:,1]","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:53:31.806699Z","iopub.execute_input":"2023-12-31T07:53:31.807179Z","iopub.status.idle":"2023-12-31T07:53:32.112548Z","shell.execute_reply.started":"2023-12-31T07:53:31.807142Z","shell.execute_reply":"2023-12-31T07:53:32.111415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_test = pd.DataFrame()\nensemble_test['logistic_reg'] = y_pred_test_lr\nensemble_test['decision_tree'] = y_pred_test_dt\nensemble_test['random_forest'] = y_pred_test_rf\nensemble_test['xgboost'] = y_pred_test_xgb\nensemble_test['naive_bayes'] = y_pred_test_gnb\nensemble_test['knn'] = y_pred_test_knn\nensemble_test['svm'] = y_prob_test_svm\nensemble_test['sequential_nn'] = y_pred_test_seq\nensemble_test['mlp_classifier'] = y_pred_test_mlp\n\n# Obtain ensemble_test predictions for validation set\nensemble_test[\"mean_proba\"] = ensemble_test.mean(axis = 1)\n\n# What cutoff to use? \nensemble_test[\"preds\"] = ensemble_test[\"mean_proba\"] >= .5\n\ntest_preds = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\n                          \"Survived\": ensemble_test[\"preds\"].astype(int)})","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:56:05.922784Z","iopub.execute_input":"2023-12-31T07:56:05.924003Z","iopub.status.idle":"2023-12-31T07:56:05.993082Z","shell.execute_reply.started":"2023-12-31T07:56:05.923953Z","shell.execute_reply":"2023-12-31T07:56:05.99145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2023-12-30T23:51:00.387981Z","iopub.execute_input":"2023-12-30T23:51:00.388581Z","iopub.status.idle":"2023-12-30T23:51:00.40195Z","shell.execute_reply.started":"2023-12-30T23:51:00.388539Z","shell.execute_reply":"2023-12-30T23:51:00.40045Z"},"trusted":true},"execution_count":null,"outputs":[]}]}